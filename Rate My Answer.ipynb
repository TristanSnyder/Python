{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Rate My Answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we pre-process answers posted in a health forum and create classifiers that predict whether an answer is posted by a medical expert (physician) or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "!pip install spacy # install spaCy\n",
    "# !pip install tqdm # install tqdm package to display the progress\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install tensorflow # install tensorflow for deep learning\n",
    "!pip install keras # install keras for deep learning\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# scikitâ€‘learn\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Keras standalone\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (\n",
    "    LSTM, Activation, Dense, Dropout, Input,\n",
    "    Embedding, Normalization, BatchNormalization\n",
    ")\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers as keras_layers\n",
    "from keras.backend import clear_session\n",
    "from keras.optimizers import *\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "# custom utilities\n",
    "from custom_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_RateMyAnswer.csv', sep = \"|\")\n",
    "test = pd.read_csv('test_RateMyAnswer.csv', sep = \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use spaCy's powerful tokenizer to parse our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new stop words: \n",
    "customize_stop_words = [\n",
    "    'user','answer'\n",
    "]\n",
    "\n",
    "# Mark them as stop words\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True\n",
    "    \n",
    "tqdm.pandas() # To display the progress\n",
    "train['pr_answer'] = train.answer.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))\n",
    "\n",
    "test['pr_answer'] = test.answer.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"label\", kind=\"count\", palette=\"ch:.25\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en_to_fr = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
    "tokenizer_en_to_fr = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
    "model_fr_to_en = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-fr-en')\n",
    "tokenizer_fr_to_en = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fr-en')\n",
    "\n",
    "def back_translate(text):\n",
    "    encoded_en = tokenizer_en_to_fr(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_tokens = model_en_to_fr.generate(**encoded_en)\n",
    "    french_text = tokenizer_en_to_fr.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    \n",
    "    encoded_fr = tokenizer_fr_to_en(french_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    back_translated_tokens = model_fr_to_en.generate(**encoded_fr)\n",
    "    back_translated_text = tokenizer_fr_to_en.batch_decode(back_translated_tokens, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return back_translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"augmented_train_set.pkl\"):\n",
    "    print(\"Augmented training set already saved. Loading from pickle...\")\n",
    "    train = pd.read_pickle(\"augmented_train_set.pkl\")\n",
    "else:\n",
    "\n",
    "    minority_df = train[train['label'] == 1]\n",
    "    \n",
    "    augmented_samples = []\n",
    "    for text in minority_df['pr_answer']:\n",
    "        augmented_text = back_translate(text)\n",
    "        augmented_samples.append(augmented_text)\n",
    "\n",
    "    augmented_df = pd.DataFrame({\n",
    "        'pr_answer': augmented_samples,\n",
    "        'label': [1] * len(augmented_samples)\n",
    "    })\n",
    "\n",
    "    train = pd.concat([train, augmented_df], ignore_index=True)\n",
    "\n",
    "    train.to_pickle(\"augmented_train_set.pkl\")\n",
    "    \n",
    "    print(\"Augmented training set created and saved.\")\n",
    "    print(\"Original data count:\", len(train))\n",
    "    print(\"Balanced data count:\", len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"label\", kind=\"count\", palette=\"ch:.25\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Learning Algorithm (LSTM):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM expects the data to be in a specific format. Therefore, instead of using the DTM, we use the original data to create sequences that are processed such that LSTM would accept them as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.pr_answer\n",
    "X_test = test.pr_answer\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "Y_train = train.label\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1) # This is a data transformation for the LSTM model \n",
    "\n",
    "Y_test = test.label\n",
    "Y_test = le.fit_transform(Y_test)\n",
    "Y_test = Y_test.reshape(-1,1) # This is a data transformation for the LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique words in pr_answer:\n",
    "from collections import Counter\n",
    "results = Counter()\n",
    "train['pr_answer'].str.lower().str.split().apply(results.update)\n",
    "print(\"Number of unique words in pr_answer:\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram of number of words in each answer:\n",
    "train['pr_answer'].str.lower().apply(lambda x: len(x.split())).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    max_words = trial.suggest_int('max_words', 5000, 20000, step=500)\n",
    "    max_len   = trial.suggest_int('max_len',   50,   200,   step=5)\n",
    "\n",
    "    tok = Tokenizer(num_words=max_words)\n",
    "    tok.fit_on_texts(X_train)\n",
    "    seq = tok.texts_to_sequences(X_train)\n",
    "    seq_matrix = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    n_layers     = trial.suggest_int('n_layers',     1,   5)\n",
    "    rnn_units    = trial.suggest_int('rnn_units',   16, 256, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    lr           = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(max_len,)))\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "    for i in range(n_layers):\n",
    "        return_seq = (i < n_layers - 1)\n",
    "        model.add(Bidirectional(LSTM(rnn_units, return_sequences=return_seq)))\n",
    "        model.add(Normalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=AdamW(learning_rate=lr)\n",
    "    )\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        seq_matrix, Y_train,\n",
    "        epochs=20, batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best = study.best_params\n",
    "print(\"Best hyperparameters:\")\n",
    "print(best)\n",
    "\n",
    "model_path = 'best_modela.pkl'\n",
    "if os.path.exists(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        final_model = pickle.load(f)\n",
    "    print(\"Loaded existing model.\")\n",
    "else:\n",
    "    tok = Tokenizer(num_words=best['max_words'])\n",
    "    tok.fit_on_texts(X_train)\n",
    "    seq = tok.texts_to_sequences(X_train)\n",
    "    seq_matrix = pad_sequences(seq, maxlen=best['max_len'], padding='post')\n",
    "\n",
    "    final_model = Sequential()\n",
    "    final_model.add(Input(shape=(best['max_len'],)))\n",
    "    final_model.add(Embedding(input_dim=best['max_words'], output_dim=64, input_length=best['max_len']))\n",
    "    for i in range(best['n_layers']):\n",
    "        return_seq = (i < best['n_layers'] - 1)\n",
    "        final_model.add(Bidirectional(LSTM(best['rnn_units'], return_sequences=return_seq)))\n",
    "        final_model.add(Normalization())\n",
    "        final_model.add(Dropout(best['dropout_rate']))\n",
    "    final_model.add(Dense(1, activation='sigmoid'))\n",
    "    final_model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=AdamW(learning_rate=best['learning_rate'])\n",
    "    )\n",
    "\n",
    "    es_final = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.003, restore_best_weights=True)\n",
    "    final_model.fit(\n",
    "        seq_matrix, Y_train,\n",
    "        epochs=50, batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[es_final],\n",
    "        class_weight={0: 1., 1: 2.},\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    print(f\"Trained and saved final model to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000 # maximum number of words to be used in the analysis\n",
    "max_len = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(X_train) # apply the tokenizer to the data\n",
    "sequences_matrix = pad_sequences(sequences,maxlen=max_len, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                            patience = 10, # Number of epochs with no improvement after which training will be stopped\n",
    "                                            min_delta=0.003, # Minimum change in the monitored quantity to qualify as an improvement\n",
    "                                            restore_best_weights=True # Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = final_model.fit(sequences_matrix, Y_train, # Data to be used for fitting/ training the model\n",
    "                    epochs=50, # Number times that the learning algorithm will work through the training data\n",
    "                    batch_size=32, # Number of samples to be used in each iteration\n",
    "                    verbose=1, # Whether to print the progress \n",
    "                    shuffle=True, # Shuffle the data for each epoch\n",
    "                    validation_split=0.2, # The portion of samples to be used for validation (different from our test data)\n",
    "                    callbacks = callback,\n",
    "                    class_weight = {0: 1.,1: 2.}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now prepare the test data for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len, padding='post') # trim or pad the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, apply the LSTM model to the test data and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictionProbabilities = final_model.predict(test_sequences_matrix).flatten()\n",
    "predictions =(final_model.predict(test_sequences_matrix) > 0.5).astype(\"int32\").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test, predictionProbabilities, pos_label = 1)\n",
    "\n",
    "lr_auc = roc_auc_score(Y_test, predictionProbabilities)\n",
    "custom_plot_roc_curve(fpr, tpr, lr_auc)\n",
    "#96.223\n",
    "#98.271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = None\n",
    "confusionMatrix = confusion_matrix(Y_test, predictions)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "displayConfusionMatrix(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predicted_score'] = predictionProbabilities\n",
    "test.sort_values(by = \"predicted_score\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Apply to Kaggle data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kaggle data:\n",
    "kaggle = pd.read_csv('kaggle_RateMyAnswer.csv', sep = \"|\")\n",
    "\n",
    "# Pre-process the answers:\n",
    "kaggle['pr_answer'] = kaggle.answer.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))\n",
    "\n",
    "# Create the sequence matrix:\n",
    "kaggle_sequences = tok.texts_to_sequences(kaggle['pr_answer'])\n",
    "kaggle_sequences_matrix = pad_sequences(kaggle_sequences,maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle['Expected'] = final_model.predict(kaggle_sequences_matrix).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle[['index','Expected']].to_csv(\"to_Kaggle_RMA.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
