{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Rate My Answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By: Dr. Reza Mousavi, University of Virginia, mousavi@virginia.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we pre-process answers posted in a health forum and create classifiers that predict whether an answer is posted by a medical expert (physician) or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "!pip install spacy # install spaCy\n",
    "# !pip install tqdm # install tqdm package to display the progress\n",
    "# !pip uninstall tensorflow -y\n",
    "# !pip install tensorflow # install tensorflow for deep learning\n",
    "!pip install keras # install keras for deep learning\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# scikitâ€‘learn\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Keras standalone\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import (\n",
    "    LSTM, Activation, Dense, Dropout, Input,\n",
    "    Embedding, Normalization, BatchNormalization\n",
    ")\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers as keras_layers\n",
    "from keras.backend import clear_session\n",
    "from keras.optimizers import *\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "# custom utilities\n",
    "from custom_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_RateMyAnswer.csv', sep = \"|\")\n",
    "test = pd.read_csv('test_RateMyAnswer.csv', sep = \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use spaCy's powerful tokenizer to parse our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new stop words: \n",
    "customize_stop_words = [\n",
    "    'user','answer'\n",
    "]\n",
    "\n",
    "# Mark them as stop words\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True\n",
    "    \n",
    "tqdm.pandas() # To display the progress\n",
    "train['pr_answer'] = train.answer.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))\n",
    "\n",
    "test['pr_answer'] = test.answer.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"label\", kind=\"count\", palette=\"ch:.25\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en_to_fr = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
    "tokenizer_en_to_fr = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
    "model_fr_to_en = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-fr-en')\n",
    "tokenizer_fr_to_en = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-fr-en')\n",
    "\n",
    "def back_translate(text):\n",
    "    encoded_en = tokenizer_en_to_fr(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_tokens = model_en_to_fr.generate(**encoded_en)\n",
    "    french_text = tokenizer_en_to_fr.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    \n",
    "    encoded_fr = tokenizer_fr_to_en(french_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    back_translated_tokens = model_fr_to_en.generate(**encoded_fr)\n",
    "    back_translated_text = tokenizer_fr_to_en.batch_decode(back_translated_tokens, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return back_translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"augmented_train_set.pkl\"):\n",
    "    print(\"Augmented training set already saved. Loading from pickle...\")\n",
    "    train = pd.read_pickle(\"augmented_train_set.pkl\")\n",
    "else:\n",
    "\n",
    "    minority_df = train[train['label'] == 1]\n",
    "    \n",
    "    augmented_samples = []\n",
    "    for text in minority_df['pr_answer']:\n",
    "        augmented_text = back_translate(text)\n",
    "        augmented_samples.append(augmented_text)\n",
    "\n",
    "    augmented_df = pd.DataFrame({\n",
    "        'pr_answer': augmented_samples,\n",
    "        'label': [1] * len(augmented_samples)\n",
    "    })\n",
    "\n",
    "    train = pd.concat([train, augmented_df], ignore_index=True)\n",
    "\n",
    "    train.to_pickle(\"augmented_train_set.pkl\")\n",
    "    \n",
    "    print(\"Augmented training set created and saved.\")\n",
    "    print(\"Original data count:\", len(train))\n",
    "    print(\"Balanced data count:\", len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"label\", kind=\"count\", palette=\"ch:.25\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Learning Algorithm (LSTM):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM expects the data to be in a specific format. Therefore, instead of using the DTM, we use the original data to create sequences that are processed such that LSTM would accept them as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.pr_answer\n",
    "X_test = test.pr_answer\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "Y_train = train.label\n",
    "Y_train = le.fit_transform(Y_train)\n",
    "Y_train = Y_train.reshape(-1,1) # This is a data transformation for the LSTM model \n",
    "\n",
    "Y_test = test.label\n",
    "Y_test = le.fit_transform(Y_test)\n",
    "Y_test = Y_test.reshape(-1,1) # This is a data transformation for the LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique words in pr_answer:\n",
    "from collections import Counter\n",
    "results = Counter()\n",
    "train['pr_answer'].str.lower().str.split().apply(results.update)\n",
    "print(\"Number of unique words in pr_answer:\", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram of number of words in each answer:\n",
    "train['pr_answer'].str.lower().apply(lambda x: len(x.split())).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    max_words = trial.suggest_int('max_words', 5000, 20000, step=500)\n",
    "    max_len   = trial.suggest_int('max_len',   50,   200,   step=5)\n",
    "\n",
    "    tok = Tokenizer(num_words=max_words)\n",
    "    tok.fit_on_texts(X_train)\n",
    "    seq = tok.texts_to_sequences(X_train)\n",
    "    seq_matrix = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    n_layers     = trial.suggest_int('n_layers',     1,   5)\n",
    "    rnn_units    = trial.suggest_int('rnn_units',   16, 256, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    lr           = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(max_len,)))\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "    for i in range(n_layers):\n",
    "        return_seq = (i < n_layers - 1)\n",
    "        model.add(Bidirectional(LSTM(rnn_units, return_sequences=return_seq)))\n",
    "        model.add(Normalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=AdamW(learning_rate=lr)\n",
    "    )\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(\n",
    "        seq_matrix, Y_train,\n",
    "        epochs=20, batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return min(history.history['val_loss'])\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best = study.best_params\n",
    "print(\"Best hyperparameters:\")\n",
    "print(best)\n",
    "\n",
    "model_path = 'best_modela.pkl'\n",
    "if os.path.exists(model_path):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        final_model = pickle.load(f)\n",
    "    print(\"Loaded existing model.\")\n",
    "else:\n",
    "    tok = Tokenizer(num_words=best['max_words'])\n",
    "    tok.fit_on_texts(X_train)\n",
    "    seq = tok.texts_to_sequences(X_train)\n",
    "    seq_matrix = pad_sequences(seq, maxlen=best['max_len'], padding='post')\n",
    "\n",
    "    final_model = Sequential()\n",
    "    final_model.add(Input(shape=(best['max_len'],)))\n",
    "    final_model.add(Embedding(input_dim=best['max_words'], output_dim=64, input_length=best['max_len']))\n",
    "    for i in range(best['n_layers']):\n",
    "        return_seq = (i < best['n_layers'] - 1)\n",
    "        final_model.add(Bidirectional(LSTM(best['rnn_units'], return_sequences=return_seq)))\n",
    "        final_model.add(Normalization())\n",
    "        final_model.add(Dropout(best['dropout_rate']))\n",
    "    final_model.add(Dense(1, activation='sigmoid'))\n",
    "    final_model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=AdamW(learning_rate=best['learning_rate'])\n",
    "    )\n",
    "\n",
    "    es_final = EarlyStopping(monitor='val_loss', patience=10, min_delta=0.003, restore_best_weights=True)\n",
    "    final_model.fit(\n",
    "        seq_matrix, Y_train,\n",
    "        epochs=50, batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[es_final],\n",
    "        class_weight={0: 1., 1: 2.},\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    print(f\"Trained and saved final model to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000 # maximum number of words to be used in the analysis\n",
    "max_len = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tok.texts_to_sequences(X_train) # apply the tokenizer to the data\n",
    "sequences_matrix = pad_sequences(sequences,maxlen=max_len, padding='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                            patience = 10, # Number of epochs with no improvement after which training will be stopped\n",
    "                                            min_delta=0.003, # Minimum change in the monitored quantity to qualify as an improvement\n",
    "                                            restore_best_weights=True # Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = final_model.fit(sequences_matrix, Y_train, # Data to be used for fitting/ training the model\n",
    "                    epochs=50, # Number times that the learning algorithm will work through the training data\n",
    "                    batch_size=32, # Number of samples to be used in each iteration\n",
    "                    verbose=1, # Whether to print the progress \n",
    "                    shuffle=True, # Shuffle the data for each epoch\n",
    "                    validation_split=0.2, # The portion of samples to be used for validation (different from our test data)\n",
    "                    callbacks = callback,\n",
    "                    class_weight = {0: 1.,1: 2.}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now prepare the test data for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X_test)\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_len, padding='post') # trim or pad the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, apply the LSTM model to the test data and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictionProbabilities = final_model.predict(test_sequences_matrix).flatten()\n",
    "predictions =(final_model.predict(test_sequences_matrix) > 0.5).astype(\"int32\").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test, predictionProbabilities, pos_label = 1)\n",
    "\n",
    "lr_auc = roc_auc_score(Y_test, predictionProbabilities)\n",
    "custom_plot_roc_curve(fpr, tpr, lr_auc)\n",
    "#96.223\n",
    "#98.271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = None\n",
    "confusionMatrix = confusion_matrix(Y_test, predictions)\n",
    "plt.rcParams['figure.figsize'] = (5, 5)\n",
    "displayConfusionMatrix(confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predicted_score'] = predictionProbabilities\n",
    "test.sort_values(by = \"predicted_score\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Apply to Kaggle data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Kaggle data:\n",
    "kaggle = pd.read_csv('kaggle_RateMyAnswer.csv', sep = \"|\")\n",
    "\n",
    "# Pre-process the answers:\n",
    "kaggle['pr_answer'] = kaggle.answer.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))\n",
    "\n",
    "# Create the sequence matrix:\n",
    "kaggle_sequences = tok.texts_to_sequences(kaggle['pr_answer'])\n",
    "kaggle_sequences_matrix = pad_sequences(kaggle_sequences,maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle['Expected'] = final_model.predict(kaggle_sequences_matrix).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle[['index','Expected']].to_csv(\"to_Kaggle_RMA.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the following article before this section: https://fortune.com/well/2023/04/04/chatgpt-advice-on-breast-cancer-screenings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to take three questions and their corresponding answers (posted on WebMD) from https://github.com/LasseRegin/medical-question-answer-data. We then ask ChatGPT to answer the same questions in 1 paragraph. In the very last example, we ask ChatGPT the same question, but we ask it to cite real studies to support its answer. We then use our model to measure the scores in these answers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans = pd.DataFrame({'question':[\"my 5 1/2-year-old son displays adhd symptoms for 20 days then for 10 days he has none. is it adhd or another condition?\",\n",
    "                                   \"my 5 1/2-year-old son displays adhd symptoms for 20 days then for 10 days he has none. is it adhd or another condition?\",\n",
    "                                   \"my son has add and mild autism. he has been successfully on concerta for 6+ years. can you help with his weight loss?\",\n",
    "                                   \"my son has add and mild autism. he has been successfully on concerta for 6+ years. can you help with his weight loss?\",\n",
    "                                   \"my son is 13 and is depressed. he has been taking vyvanse for the last 3 years and never had a problem. can you help?\",\n",
    "                                   \"my son is 13 and is depressed. he has been taking vyvanse for the last 3 years and never had a problem. can you help?\",\n",
    "                                   \"my son is 13 and is depressed. he has been taking vyvanse for the last 3 years and never had a problem. can you help?\"],\n",
    "                       \n",
    "                       'answer':[\"adhd and bipolar mood disorder (bmd) can coexist or be mistaken for one another. bmd usually is not diagnosed until young adulthood. however studies have shown that the earlier a person is diagnosed with bmd the more likely he is to have been diagnosed with adhd previously. in this case i would just like to reiterate that there is not enough information to discuss either possibility for your son. you mentioned that he becomes hyperactive for 3 weeks but not what his behaviors are like during those 10 days. you also do not mention irritability or mood swings just adhd symptoms. keep documenting the symptoms you are concerned about including what goes on in the home and at school when you see changes in behavior (do you work those weeks does he visit a relative or have a different aide in the classroom). you also mentioned that this began 7 months ago. i would also urge you to think about what also might have changed in your son's life about that time. consulting your pediatrician or a child psychiatrist should help you unravel the mystery of your son's behavior.\",\n",
    "                                 \"ADHD is typically characterized by consistent symptoms over time, but they can vary in different situations. It is possible that other factors, such as changes in routine, environment, or stress levels, could contribute to the fluctuation in your son's symptoms. To determine whether your son has ADHD or another condition, it is essential to consult with a healthcare professional, such as a pediatrician or child psychologist, who can provide an accurate diagnosis and appropriate guidance based on a thorough evaluation.\",\n",
    "                                 \"stimulants in general tend to decrease appetite in certain not all children. over time this side effect seems to lessen but as in the case of your son for some children it may persist. weight loss is the result of this appetite decrease and not consuming enough calories during the day. therefore since the medication is effective and at \\\"the perfect dose for him \\\" right now i would suggest that you work on increasing your son's calorie intake. this however is often not an easy task! some suggestions include offering your son his preferred foods whenever he seems hungry; starting the day with a large calorie-laden breakfast early in the morning before the medication takes effect; increasing the calories in snacks during the day and at bedtime; and adding a high calorie supplement like ensure or shakes with ice cream etc. during the day. consult your pediatrician or a nutrionist to determine how many calories a day your son should be consuming and try to make sure this is the minimum he consumes to prevent weight loss. if you find this remains a problem i suggest you consult a pediatric nutritionist for additional suggestions. if the problems persists despite you and your son's best efforts you may want to discuss changing to a non-stimulant medication with your son's prescribing physician.\",\n",
    "                                 \"Weight loss in children taking Concerta can be a side effect of the medication, as it may suppress appetite. To address this issue, consider offering nutritious, calorie-dense meals and snacks throughout the day, focusing on foods rich in healthy fats, proteins, and complex carbohydrates. Encourage regular mealtimes, and try to make eating enjoyable and stress-free. It's important to closely monitor your son's weight and growth, and consult with his healthcare provider for personalized recommendations and adjustments to his medication or treatment plan, if needed.\",\n",
    "                                 \"while any of the stimulant medications can increase depressive symptoms. it's important to pay attention to this change in your son's mood and take him to a mental health professional for an assessment. children diagnosed with attention deficit/hyperactivitydisorder (adhd) at early are more likely to suffer from depression as adolescents than those who did not have adhd according to a long-term study published in the october issue of the archives of general psychiatry. although it was an uncommon occurrence the children with adhd also were somewhat more likely to think about or attempt suicide as adolescents. the study performed by researchers at the university of chicago and the university of pittsburgh followed 123 children diagnosed with adhd at age 4 to 6 for up to 14 years until they reached ages 18 to 20. it compared them with 119 children from similar neighborhoods and schools matched for age sex and ethnicity. the researchers found that 18 percent of children diagnosed early with adhd suffered from depression as adolescents about 10 times the rate among those without adhd. children with early adhd were five times as likely to have considered suicide at least once and twice as likely to have made an attempt. please get your son evaluated.\",\n",
    "                                 \"If your son is experiencing depression while taking Vyvanse, it's crucial to consult with his healthcare provider to discuss his symptoms and evaluate whether a change in medication or dosage is necessary. Additionally, consider exploring other forms of support, such as therapy, counseling, or support groups, which can help address his emotional well-being. Encourage open communication, provide a supportive environment, and monitor any changes in his mood or behavior. Remember, it's essential to work closely with healthcare professionals to develop the most appropriate treatment plan for your son's needs.\",\n",
    "                                 \"A study by Coghill et al. (2014) suggests that ADHD medications, including Vyvanse, can sometimes be associated with mood-related side effects. If your son is experiencing depression while taking Vyvanse, it is important to consult with his healthcare provider to discuss his symptoms and evaluate whether a change in medication or dosage is necessary. Additionally, consider exploring other forms of support, such as therapy, counseling, or support groups, which can help address his emotional well-being (Weersing et al., 2017). It's essential to work closely with healthcare professionals to develop the most appropriate treatment plan for your son's needs.\"],\n",
    "                      'label':[\"webmd\",\"chatgpt\",\"webmd\",\"chatgpt\",\"webmd\",\"chatgpt\",\"chatgptCITE\"]})\n",
    "gp_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the answers:\n",
    "gp_ans['pr_answer'] = gp_ans.answer.progress_apply(lambda text: \n",
    "                                          \" \".join(token.lemma_ for token in nlp(text) \n",
    "                                                   if not token.is_stop and token.is_alpha))\n",
    "\n",
    "# Create the sequence matrix:\n",
    "gp_ans_sequences = tok.texts_to_sequences(gp_ans['pr_answer'])\n",
    "gp_ans_sequences_matrix = pad_sequences(gp_ans_sequences,maxlen=max_len, padding='post')\n",
    "\n",
    "gp_ans['Expected'] = model.predict(gp_ans_sequences_matrix).flatten()\n",
    "gp_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans.question.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans.answer.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans.answer.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans.question.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans.answer.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans.answer.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_ans.answer.iloc[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References in the ChatGPT Answer:\n",
    "\n",
    "Coghill, D., Banaschewski, T., Lecendreux, M., Soutullo, C., Johnson, M., Zuddas, A., ... & Squires, L. (2014). European, randomized, phase 3 study of lisdexamfetamine dimesylate in children and adolescents with attention-deficit/hyperactivity disorder. European Neuropsychopharmacology, 24(10), 1661-1672.\n",
    "\n",
    "Weersing, V. R., Jeffreys, M., Do, M. T., Schwartz, K. T., & Bolano, C. (2017). Evidence Base Update of Psychosocial Treatments for Child and Adolescent Depression. Journal of Clinical Child & Adolescent Psychology, 46(1), 11-43."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
