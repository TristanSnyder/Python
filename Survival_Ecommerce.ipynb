{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Notebook Styling and Library Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgbse\n",
      "  Using cached xgbse-0.3.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting joblib<2.0.0,>=1.4.2 (from xgbse)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting lifelines<0.30.0,>=0.29.0 (from xgbse)\n",
      "  Using cached lifelines-0.29.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /opt/conda/lib/python3.11/site-packages (from xgbse) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from xgbse) (2.2.3)\n",
      "Collecting scikit-learn<2.0.0,>=1.5.0 (from xgbse)\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: xgboost<3.0.0,>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from xgbse) (2.1.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from lifelines<0.30.0,>=0.29.0->xgbse) (1.11.4)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/conda/lib/python3.11/site-packages (from lifelines<0.30.0,>=0.29.0->xgbse) (3.9.1)\n",
      "Requirement already satisfied: autograd>=1.5 in /opt/conda/lib/python3.11/site-packages (from lifelines<0.30.0,>=0.29.0->xgbse) (1.7.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /opt/conda/lib/python3.11/site-packages (from lifelines<0.30.0,>=0.29.0->xgbse) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /opt/conda/lib/python3.11/site-packages (from lifelines<0.30.0,>=0.29.0->xgbse) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.0->xgbse) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.0->xgbse) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.0->xgbse) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=1.5.0->xgbse) (3.5.0)\n",
      "Collecting nvidia-nccl-cu12 (from xgboost<3.0.0,>=2.1.0->xgbse)\n",
      "  Using cached nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines<0.30.0,>=0.29.0->xgbse) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines<0.30.0,>=0.29.0->xgbse) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.0 in /opt/conda/lib/python3.11/site-packages (from formulaic>=0.2.2->lifelines<0.30.0,>=0.29.0->xgbse) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines<0.30.0,>=0.29.0->xgbse) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines<0.30.0,>=0.29.0->xgbse) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines<0.30.0,>=0.29.0->xgbse) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines<0.30.0,>=0.29.0->xgbse) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines<0.30.0,>=0.29.0->xgbse) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines<0.30.0,>=0.29.0->xgbse) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.0->lifelines<0.30.0,>=0.29.0->xgbse) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.0->xgbse) (1.16.0)\n",
      "Using cached xgbse-0.3.3-py3-none-any.whl (35 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached lifelines-0.29.0-py3-none-any.whl (349 kB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.25.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.4 MB)\n",
      "Installing collected packages: nvidia-nccl-cu12, joblib, scikit-learn, lifelines, xgbse\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.3.2\n",
      "    Uninstalling joblib-1.3.2:\n",
      "      Successfully uninstalled joblib-1.3.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.4.2\n",
      "    Uninstalling scikit-learn-1.4.2:\n",
      "      Successfully uninstalled scikit-learn-1.4.2\n",
      "  Attempting uninstall: lifelines\n",
      "    Found existing installation: lifelines 0.30.0\n",
      "    Uninstalling lifelines-0.30.0:\n",
      "      Successfully uninstalled lifelines-0.30.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "pycaret 3.3.1 requires joblib<1.4,>=1.2.0, but you have joblib 1.4.2 which is incompatible.\n",
      "pycaret 3.3.1 requires matplotlib<3.8.0, but you have matplotlib 3.9.1 which is incompatible.\n",
      "pycaret 3.3.1 requires pandas<2.2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "scikit-survival 0.23.1 requires scikit-learn<1.6,>=1.4.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "sktime 0.26.0 requires pandas<2.2.0,>=1.1, but you have pandas 2.2.3 which is incompatible.\n",
      "sktime 0.26.0 requires scikit-learn<1.5.0,>=0.24, but you have scikit-learn 1.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.4.2 lifelines-0.29.0 nvidia-nccl-cu12-2.25.1 scikit-learn-1.6.1 xgbse-0.3.3\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c sebp scikit-survival --yes \n",
    "# !pip install lifelines\n",
    "!pip install xgbse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feature-engine in /opt/conda/lib/python3.11/site-packages (1.8.3)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.11/site-packages (from feature-engine) (1.26.4)\n",
      "Requirement already satisfied: pandas>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from feature-engine) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from feature-engine) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.11/site-packages (from feature-engine) (1.11.4)\n",
      "Requirement already satisfied: statsmodels>=0.11.1 in /opt/conda/lib/python3.11/site-packages (from feature-engine) (0.14.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.2.0->feature-engine) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.2.0->feature-engine) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=2.2.0->feature-engine) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.4.0->feature-engine) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.4.0->feature-engine) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/conda/lib/python3.11/site-packages (from statsmodels>=0.11.1->feature-engine) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.11/site-packages (from statsmodels>=0.11.1->feature-engine) (24.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from patsy>=0.5.6->statsmodels>=0.11.1->feature-engine) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install feature-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optunahub in /opt/conda/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: ga4mp in /opt/conda/lib/python3.11/site-packages (from optunahub) (2.0.4)\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.11/site-packages (from optunahub) (4.2.1)\n",
      "Requirement already satisfied: PyGithub>=1.59 in /opt/conda/lib/python3.11/site-packages (from optunahub) (2.6.0)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from PyGithub>=1.59->optunahub) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in /opt/conda/lib/python3.11/site-packages (from PyGithub>=1.59->optunahub) (2.32.3)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub>=1.59->optunahub) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from PyGithub>=1.59->optunahub) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from PyGithub>=1.59->optunahub) (2.2.2)\n",
      "Requirement already satisfied: Deprecated in /opt/conda/lib/python3.11/site-packages (from PyGithub>=1.59->optunahub) (1.2.18)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna->optunahub) (1.13.2)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from optuna->optunahub) (6.9.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna->optunahub) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna->optunahub) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna->optunahub) (2.0.31)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna->optunahub) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna->optunahub) (6.0.1)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna->optunahub) (1.3.5)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub>=1.59->optunahub) (43.0.0)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.11/site-packages (from pynacl>=1.4.0->PyGithub>=1.59->optunahub) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.14.0->PyGithub>=1.59->optunahub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.14.0->PyGithub>=1.59->optunahub) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.14.0->PyGithub>=1.59->optunahub) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna->optunahub) (3.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from Deprecated->PyGithub>=1.59->optunahub) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub>=1.59->optunahub) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->optuna->optunahub) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optunahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Library for math operations\n",
    "import pandas as pd # Library for data handling\n",
    "from sksurv.nonparametric import kaplan_meier_estimator # Library for survival analysis\n",
    "import matplotlib.pyplot as plt # Library for plotting\n",
    "import seaborn as sns # Another library for plotting\n",
    "plt.style.use('fivethirtyeight') # Set the styling to FiveThirtyEight setting.\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read & Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainInput = pd.read_csv('trainInput.csv')\n",
    "testInput = pd.read_csv('testInput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.read_csv('kaggle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainInput.drop(columns = ['id','purchased', 'days_on_market'])\n",
    "trainLabels = trainInput['purchased']\n",
    "testData = testInput.drop(columns = ['id','purchased', 'days_on_market'])\n",
    "testLabels = testInput['purchased']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## 3. Data Augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData[['product_id']] = trainData[['product_id']].astype(str)\n",
    "\n",
    "testData[['product_id']] = testData[['product_id']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Explanation and Design Choices\n",
    "\n",
    "This code is designed for survival analysis using an **XGBoost-based Survival Model (XGBSE)**, optimized with **Optuna** for hyperparameter tuning. It includes a detailed **preprocessing pipeline**, **feature engineering**, and **model selection** steps.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components and Why They Are Used**\n",
    "\n",
    "#### **1. Hyperparameter Optimization with Optuna**\n",
    "- **Why?** Automates the search for optimal hyperparameters, ensuring the best performance for the survival model.\n",
    "- **How?** It defines an `objective` function that tries different preprocessing settings and model parameters, and evaluates them using **Concordance Index**.\n",
    "\n",
    "#### **2. Preprocessing Pipeline**\n",
    "The pipeline ensures the input data is clean and well-processed before being fed into the model.\n",
    "\n",
    "- **Outlier Handling (Winsorization)**  \n",
    "  - Uses `Winsorizer()` to limit extreme values in both tails or a specific tail.\n",
    "  - **Why?** Controls the impact of outliers on the model.\n",
    "  \n",
    "- **Categorical Encoding**\n",
    "  - `RareLabelEncoder()`: Groups infrequent categories into \"rare\" labels.\n",
    "  - `WoEEncoder()`: Converts categorical variables to numerical values using **Weight of Evidence (WoE)** (optional).\n",
    "  - **Why?** Reduces sparsity and improves model interpretability.\n",
    "\n",
    "- **Feature Selection**\n",
    "  - `DropConstantFeatures()`: Removes features with little variation (near-constant).\n",
    "  - **Why?** Reduces redundancy and computational cost.\n",
    "\n",
    "- **Feature Engineering**\n",
    "  - `MathFeatures()`: Creates interaction terms like products of selected feature pairs.\n",
    "  - **Why?** Enhances feature representation for better predictive power.\n",
    "\n",
    "- **Label Encoding for Categorical Features**\n",
    "  - Uses `LabelEncoder()` on categorical columns before feature transformations.\n",
    "  - **Why?** Converts categorical values into numerical form, necessary for models that don’t handle categorical data natively.\n",
    "\n",
    "- **MinMax Scaling**\n",
    "  - Applies `MinMaxScaler()` to scale all features between 0 and 1.\n",
    "  - **Why?** Ensures consistency across different feature ranges.\n",
    "\n",
    "#### **3. Survival Model Setup**\n",
    "- Uses **XGBSEStackedWeibull** as the base model, wrapped in **XGBSEBootstrapEstimator**.\n",
    "- **Why these models?**\n",
    "  - **XGBSEStackedWeibull**: Extends XGBoost for survival analysis, leveraging a **Weibull distribution**.\n",
    "  - **XGBSEBootstrapEstimator**: Reduces variance by using **bootstrap resampling**.\n",
    "\n",
    "- Converts labels using `convert_to_structured()`, necessary for survival analysis.\n",
    "- **Why?** Survival models need structured time-to-event data instead of simple class labels.\n",
    "\n",
    "#### **4. Model Training and Evaluation**\n",
    "- **Hyperparameter tuning for the survival model includes:**\n",
    "  - `learning_rate`: Controls step size (log-scaled for fine control).\n",
    "  - `max_depth`: Controls tree depth for complexity.\n",
    "  - `booster`: Chooses between \"gbtree\" (default) and \"dart\" (dropout trees for regularization).\n",
    "  - `subsample`: Selects a subset of data to reduce overfitting.\n",
    "  - `min_child_weight`: Controls minimum sum of instance weights per leaf.\n",
    "  - `colsample_bynode`: Controls feature sampling for diversity.\n",
    "\n",
    "- Uses **early stopping** to prevent overfitting by monitoring validation performance.\n",
    "\n",
    "- **Evaluation Metric: Concordance Index (C-index)**\n",
    "  - Measures how well the model ranks survival times.\n",
    "  - **Why?** Ideal for survival analysis as it focuses on ranking rather than absolute time predictions.\n",
    "\n",
    "#### **5. Model Persistence (Pickle)**\n",
    "- Saves the trained model (`xgbse_model.pkl`) and preprocessing pipeline (`preprocessing_pipeline.pkl`) for future inference.\n",
    "- **Why?** Avoids retraining the model every time predictions are needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why This Structure?**\n",
    "1. **Automated Hyperparameter Optimization**  \n",
    "   - Optuna systematically tunes preprocessing and model parameters to maximize performance.\n",
    "\n",
    "2. **Comprehensive Preprocessing**  \n",
    "   - Handles outliers, rare categories, feature selection, encoding, and scaling.\n",
    "\n",
    "3. **Feature Engineering to Enhance Predictive Power**  \n",
    "   - Introduces interaction terms via `MathFeatures()`.\n",
    "\n",
    "4. **Survival-Specific Label Conversion**  \n",
    "   - Uses `convert_to_structured()` to properly format target labels.\n",
    "\n",
    "5. **Bootstrap-Based Survival Modeling**  \n",
    "   - Reduces variance by aggregating multiple model predictions.\n",
    "\n",
    "6. **Scalability and Reproducibility**  \n",
    "   - Saves preprocessing steps and models for easy deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "This code builds an **optimized survival analysis model** using XGBoost and **XGBSE**, enhanced with automated preprocessing and feature selection. It leverages **Optuna for tuning** and **bootstrap ensembling** for stability, ensuring robust predictions in time-to-event modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-20 23:27:11,402] A new study created in memory with name: no-name-dadd1e9e-3cb3-4193-b8dd-e49a6c02891a\n",
      "[I 2025-02-20 23:29:12,790] Trial 0 finished with value: 0.7582376332910143 and parameters: {'winsor_tail': 'left', 'winsor_limits': 0.0848653038673566, 'rare_tol': 0.008852925609886764, 'rare_n_categories': 4, 'drop_tol': 0.969657634376374, 'apply_woe': False, 'use_math_features': False, 'math_vars': None, 'learning_rate': 0.0637679969690931, 'max_depth': 9, 'booster': 'gbtree', 'subsample': 0.5756872586802466, 'min_child_weight': 119, 'colsample_bynode': 0.726708103197294, 'n_estimators': 150, 'num_boost_round': 50}. Best is trial 0 with value: 0.7582376332910143.\n",
      "[I 2025-02-20 23:31:52,066] Trial 1 finished with value: 0.7541729214230058 and parameters: {'winsor_tail': 'both', 'winsor_limits': 0.07022379718733353, 'rare_tol': 0.009176405409707777, 'rare_n_categories': 2, 'drop_tol': 0.9592582829844837, 'apply_woe': True, 'use_math_features': True, 'math_vars': None, 'learning_rate': 0.03683818903057118, 'max_depth': 3, 'booster': 'gbtree', 'subsample': 0.8977196901982678, 'min_child_weight': 32, 'colsample_bynode': 0.6948905582928899, 'n_estimators': 150, 'num_boost_round': 180}. Best is trial 0 with value: 0.7582376332910143.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from xgbse import (\n",
    "    XGBSEKaplanNeighbors, XGBSEDebiasedBCE, XGBSEKaplanTree,\n",
    "    XGBSEStackedWeibull, XGBSEBootstrapEstimator\n",
    ")\n",
    "from xgbse.converters import convert_to_structured\n",
    "from xgbse.metrics import (\n",
    "    concordance_index, approx_brier_score, dist_calibration_score\n",
    ")\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.encoding import RareLabelEncoder, WoEEncoder\n",
    "from feature_engine.selection import DropConstantFeatures\n",
    "from feature_engine.creation import MathFeatures\n",
    "\n",
    "# Create Storage for Preprocessing Steps and Model\n",
    "preprocessing_steps = {}\n",
    "trained_models = {}  # Store trained model here\n",
    "\n",
    "def objective(trial):\n",
    "    global preprocessing_steps, trained_models  # Store best transformations and model\n",
    "\n",
    "    # Preprocessing Hyperparameters\n",
    "\n",
    "    # 1. Winsorization (Handle Outliers)\n",
    "    winsor_tail = trial.suggest_categorical(\"winsor_tail\", [\"both\", \"right\", \"left\"])\n",
    "    winsor_limits = trial.suggest_float(\"winsor_limits\", 0.05, 0.1)\n",
    "    out = Winsorizer(tail=winsor_tail, fold=winsor_limits)\n",
    "\n",
    "    # 2. Rare Label Encoding\n",
    "    rare_tol = trial.suggest_float(\"rare_tol\", 0.0075, 0.01)\n",
    "    rare_n_categories = trial.suggest_int(\"rare_n_categories\", 1, 5)\n",
    "    enc = RareLabelEncoder(tol=rare_tol, n_categories=rare_n_categories)\n",
    "\n",
    "    # 3. Feature Selection (Drop Constant Features)\n",
    "    drop_tol = trial.suggest_float(\"drop_tol\", 0.95, 0.98)\n",
    "    con = DropConstantFeatures(tol=drop_tol)\n",
    "\n",
    "    # 4. Weight of Evidence Encoding (Can be turned ON or OFF)\n",
    "    apply_woe = trial.suggest_categorical(\"apply_woe\", [True, False])\n",
    "    enc2 = WoEEncoder() if apply_woe else None\n",
    "\n",
    "    # 5. Feature Engineering (Math Features)\n",
    "    use_math_features = trial.suggest_categorical(\"use_math_features\", [True, False])\n",
    "    possible_math_vars = [[\"brand\", \"product_id\"], [\"price\", \"color\"]]\n",
    "    \n",
    "    # Convert feature lists to strings for Optuna\n",
    "    math_vars_options = [\",\".join(vars) for vars in possible_math_vars] + [None]\n",
    "    selected_math_vars = trial.suggest_categorical(\"math_vars\", math_vars_options)\n",
    "\n",
    "    # Convert back to list (or None) for MathFeatures\n",
    "    math_vars = selected_math_vars.split(\",\") if selected_math_vars else None\n",
    "    mf = MathFeatures(variables=math_vars, func=[\"prod\"]) if use_math_features and math_vars else None\n",
    "\n",
    "    # Apply Preprocessing Steps\n",
    "    train_trans, test_trans = trainData.copy(), testData.copy()\n",
    "    \n",
    "    train_trans, test_trans = out.fit_transform(train_trans), out.transform(test_trans)\n",
    "    train_trans, test_trans = enc.fit_transform(train_trans), enc.transform(test_trans)\n",
    "    train_trans, test_trans = con.fit_transform(train_trans), con.transform(test_trans)\n",
    "    \n",
    "    if apply_woe:\n",
    "        train_trans, test_trans = enc2.fit_transform(train_trans, trainLabels), enc2.transform(test_trans)\n",
    "\n",
    "    # Convert Categorical Features to Numeric Before Feature Engineering\n",
    "    label_encoders = {}\n",
    "    for col in train_trans.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder()\n",
    "        train_trans[col] = le.fit_transform(train_trans[col])\n",
    "        test_trans[col] = le.transform(test_trans[col])\n",
    "        label_encoders[col] = le  \n",
    "\n",
    "    # Apply Math Features AFTER Encoding\n",
    "    if use_math_features and math_vars and all(var in train_trans.columns for var in math_vars):\n",
    "        train_trans, test_trans = mf.fit_transform(train_trans), mf.transform(test_trans)\n",
    "\n",
    "    # MinMax Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    train_trans = pd.DataFrame(scaler.fit_transform(train_trans), columns=train_trans.columns)\n",
    "    test_trans = pd.DataFrame(scaler.transform(test_trans), columns=test_trans.columns)\n",
    "\n",
    "    # Store Fitted Transformers for Later Use\n",
    "    preprocessing_steps[\"winsorizer\"] = out\n",
    "    preprocessing_steps[\"rare_encoder\"] = enc\n",
    "    preprocessing_steps[\"drop_constant\"] = con\n",
    "    preprocessing_steps[\"scaler\"] = scaler\n",
    "    preprocessing_steps[\"label_encoders\"] = label_encoders\n",
    "    if apply_woe:\n",
    "        preprocessing_steps[\"woe_encoder\"] = enc2\n",
    "    if use_math_features and math_vars:\n",
    "        preprocessing_steps[\"math_features\"] = mf\n",
    "\n",
    "    # Convert Labels for Survival Model\n",
    "    y_train = convert_to_structured(trainInput[\"days_on_market\"], trainLabels)\n",
    "    y_val = convert_to_structured(testInput[\"days_on_market\"], testLabels)\n",
    "\n",
    "    # Model Hyperparameters\n",
    "    params = {\n",
    "        \"objective\": \"survival:cox\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 0.9),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 10, 200),\n",
    "        \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.4, 0.9),\n",
    "    }\n",
    "\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500, step=50)\n",
    "    num_boost_round = trial.suggest_int(\"num_boost_round\", 10, 200, step=10)\n",
    "\n",
    "    # Create the base model\n",
    "    base = XGBSEStackedWeibull(params)\n",
    "    xgbse_model = XGBSEBootstrapEstimator(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=1\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    xgbse_model.fit(\n",
    "        train_trans, y_train,\n",
    "        num_boost_round=num_boost_round,\n",
    "        validation_data=(test_trans, y_val),\n",
    "        early_stopping_rounds=20,  \n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Store trained model for later use\n",
    "    trained_models[\"xgbse_model\"] = xgbse_model  \n",
    "\n",
    "    # Predict\n",
    "    preds = xgbse_model.predict(test_trans)\n",
    "\n",
    "    # Evaluate using Concordance Index\n",
    "    return concordance_index(y_val, preds)\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")  # Maximize C-index\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Best parameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Save preprocessing steps and trained model for later use\n",
    "with open(\"preprocessing_pipeline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessing_steps, f)\n",
    "\n",
    "with open(\"xgbse_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trained_models[\"xgbse_model\"], f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script loads a pre-trained XGBSE survival model and its preprocessing pipeline, applies transformations to new data, generates predictions, and prepares a submission file. It ensures consistency with training by reusing preprocessing steps like outlier handling, encoding, feature selection, and scaling. Predictions are formatted correctly for submission, making the workflow efficient, reproducible, and deployment-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load Preprocessing Steps\n",
    "with open(\"preprocessing_pipeline.pkl\", \"rb\") as f:\n",
    "    preprocessing_steps = pickle.load(f)\n",
    "\n",
    "# Load the Trained Model\n",
    "with open(\"xgbse_model.pkl\", \"rb\") as f:\n",
    "    trained_models = pickle.load(f)\n",
    "\n",
    "print(\"Preprocessing pipeline and model loaded successfully!\")\n",
    "\n",
    "# Apply Preprocessing to New Data\n",
    "def apply_preprocessing(new_data, preprocessing_steps):\n",
    "    \"\"\"Applies the saved preprocessing steps to new data before inference.\"\"\"\n",
    "    new_data = preprocessing_steps[\"winsorizer\"].transform(new_data)\n",
    "    new_data = preprocessing_steps[\"rare_encoder\"].transform(new_data)\n",
    "    new_data = preprocessing_steps[\"drop_constant\"].transform(new_data)\n",
    "    # Apply Label Encoding for categorical columns\n",
    "    for col, le in preprocessing_steps[\"label_encoders\"].items():\n",
    "        new_data[col] = le.transform(new_data[col])\n",
    "    if \"woe_encoder\" in preprocessing_steps:\n",
    "        new_data = preprocessing_steps[\"woe_encoder\"].transform(new_data)\n",
    "    \n",
    "    if \"math_features\" in preprocessing_steps:\n",
    "        new_data = preprocessing_steps[\"math_features\"].fit_transform(new_data)\n",
    "\n",
    "    # Apply Label Encoding for categorical columns\n",
    "    #for col, le in preprocessing_steps[\"label_encoders\"].items():\n",
    "     #   new_data[col] = le.transform(new_data[col])\n",
    "\n",
    "    # Apply MinMax Scaling\n",
    "    new_data = pd.DataFrame(preprocessing_steps[\"scaler\"].transform(new_data), columns=new_data.columns)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Apply preprocessing to kaggle\n",
    "kaggle_transformed = apply_preprocessing(kaggle, preprocessing_steps)\n",
    "\n",
    "# If labels are available, convert them\n",
    "if 'days_on_market' in kaggle.columns:\n",
    "    kaggle_labels = convert_to_structured(kaggle[\"days_on_market\"], kaggleLabels)\n",
    "\n",
    "print(\"kaggle data preprocessed successfully. Ready for predictions!\")\n",
    "\n",
    "# Prepare Submission\n",
    "submission = kaggle.copy()\n",
    "submission[['product_id']] = submission[['product_id']].astype(str)\n",
    "\n",
    "submission['Expected'] = 1 - trained_models.predict(kaggle_transformed)[31]\n",
    "submission['Id'] = submission.index.astype(str)\n",
    "\n",
    "# Save to CSV\n",
    "submission[['Id', 'Expected']].to_csv('to_kaggle.csv', index=False)\n",
    "print(\"Submission file 'to_kaggle.csv' saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit to: https://www.kaggle.com/t/8a2e03e370c74cafbb28375aed425682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBSE Docs:\n",
    "\n",
    "https://github.com/loft-br/xgboost-survival-embeddings/blob/main/docs/how_xgbse_works.md"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
